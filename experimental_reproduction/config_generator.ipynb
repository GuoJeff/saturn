{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saturn Run Configuration Generator\n",
    "\n",
    "This `Jupyter` notebook introduces each component of the run configuration for `Saturn`. Running the notebook itself will output the `JSON` which can then be run directly via:\n",
    "\n",
    "**python saturn.py `JSON`**\n",
    "\n",
    "**Note**: The syntax of the configuration `JSON` is similar to `REINVENT 3.2`: https://github.com/MolecularAI/Reinvent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths denoted \"FILL\" should be modified accordingly\n",
    "\n",
    "config = {\n",
    "  \"logging\": {\n",
    "    \"logging_path\": \"FILL\",  # Denotes where to save the output log file\n",
    "    \"model_checkpoints_dir\": \"FILL\"  # Denotes the directory to save the model checkpoints\n",
    "  },\n",
    "  \"oracle\": {\n",
    "    \"budget\": 1000,  # Denotes the number of *unique* oracle calls permitted\n",
    "    \"allow_oracle_repeats\": False,  # Denotes whether to allow oracle repeats - if this is True, using Mamba will result in lots of repeated oracle calls\n",
    "    \"aggregator\": \"product\",  # Denotes the reward aggregator - relevant with > 1 oracle components. See oracles/reward_aggregator/reward_aggregator.py for more details\n",
    "\n",
    "    # Each one of the blocks below denotes an oracle component to optimize for - there is no limit to the number of simultaneous objectives but \n",
    "    # too many will make optimization difficult due to reward sparsity. In a situation like this, curriculum learning could be applied: https://www.nature.com/articles/s42256-022-00494-4\n",
    "    # For a list of supported oracle components, see oracles/utils.py\n",
    "    # Below are details on the key parameters:\n",
    "    # \"weight\":weight of the oracle component - higher makes the reward contribution from the component more important\n",
    "    # \"preliminary_check\": whether to run this specific oracle component first and if the reward does not pass a threshold, discard the SMILES. \n",
    "    #                       This is useful for components that are computationally inexpensive as a way to \"pre-screen\" the batch and not waste oracle calls \n",
    "    # \"reward_shaping_function_parameters\": reward shaping function to apply to the component. The syntax is exactly the same as REINVENT 3.2. \n",
    "    #                                       See the following notebook for function visualizations:nhttps://github.com/MolecularAI/ReinventCommunity/blob/master/notebooks/Score_Transformations.ipynb\n",
    "    \"components\": [\n",
    "      {\n",
    "        \"name\": \"tpsa\",\n",
    "        \"weight\": 1,\n",
    "        \"preliminary_check\": False,\n",
    "        \"specific_parameters\": {},\n",
    "        \"reward_shaping_function_parameters\": {\n",
    "          \"transformation_function\": \"sigmoid\",\n",
    "          \"parameters\": {\n",
    "            \"low\": 75,\n",
    "            \"high\": 350,\n",
    "            \"k\": 0.15\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"mw\",\n",
    "        \"weight\": 1,\n",
    "        \"preliminary_check\": False,\n",
    "        \"specific_parameters\": {},\n",
    "        \"reward_shaping_function_parameters\": {\n",
    "          \"transformation_function\": \"double_sigmoid\",\n",
    "          \"parameters\": {\n",
    "            \"low\": 0,\n",
    "            \"high\": 350,\n",
    "            \"coef_div\": 500,\n",
    "            \"coef_si\": 250,\n",
    "            \"coef_se\": 250\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"num_rings\",\n",
    "        \"weight\": 1,\n",
    "        \"preliminary_check\": False,\n",
    "        \"specific_parameters\": {},\n",
    "        \"reward_shaping_function_parameters\": {\n",
    "          \"transformation_function\": \"step\",\n",
    "          \"parameters\": {\n",
    "            \"low\": 2,\n",
    "            \"high\": 5\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  \"goal_directed_generation\": {\n",
    "    \"reinforcement_learning\": {\n",
    "      \"prior\": \"FILL\",  # Path to pre-trained model\n",
    "      \"agent\": \"FILL\",  # Usually same as prior\n",
    "      # The parameters below are the optimal found in the paper, but can be adjusted.\n",
    "      \"batch_size\": 16,\n",
    "      \"learning_rate\": 0.0001,\n",
    "      \"sigma\": 128.0,\n",
    "      \"augmented_memory\": True,\n",
    "      \"augmentation_rounds\": 10,\n",
    "      \"selective_memory_purge\": True  # *Highly* recommended that this is kept True - otherwise detrimental mode collapse rapidly occurs, especially with augmentation rounds > 2\n",
    "    },\n",
    "    \"experience_replay\": {\n",
    "      \"memory_size\": 100,  # Maximum size of Replay Buffer\n",
    "      \"sample_size\": 10,  # Number of SMILES to sample for experience replay - this is independent of Augmented Memory which will always sample the full memory\n",
    "      \"smiles\": []  # Optionally seed the Replay Buffer with known positive examples - this is known as \"Inception\" in REINVENT\n",
    "    },\n",
    "    # Based on REINVENT\n",
    "    \"diversity_filter\": {\n",
    "      \"name\": \"IdenticalMurckoScaffold\",  # This is not used at the moment - by default, scaffolds are Bemis-Murcko (considers heavy atoms)\n",
    "      \"bucket_size\": 10  # Maximum number of times a Bemis-Murcko scaffold can be sampled before reward is truncated to 0\n",
    "    },\n",
    "    # This block controls whether to active the genetic algorithm on the Replay Buffer\n",
    "    \"hallucinated_memory\": {\n",
    "      \"execute_hallucinated_memory\": False,  # True will execute the genetic algorithm\n",
    "      \"hallucination_method\": \"ga\",  # Technically, \"sequence\" and \"ga\" are supported, but \"sequence\" is unused at the moment\n",
    "      \"num_hallucinations\": 100,  # Number of SMILES to hallucinate\n",
    "      \"num_selected\": 5,  # Number of SMILES to select from the total hallucinations\n",
    "      \"selection_criterion\": \"random\"  # How to select the hallucinations - \"random\" selects randomly while \"tanimoto_distance\" selects based on max *dissimilarity* to the Replay Buffer\n",
    "    },\n",
    "    # This block controls whether to run Beam Enumeration\n",
    "    # NOTE: This is currently only implemented for the \"rnn\" model architecture\n",
    "    # See https://arxiv.org/abs/2309.13957 for details on the parameters\n",
    "    \"beam_enumeration\": {\n",
    "      \"execute_beam_enumeration\": False,  # True will execute Beam Enumeration\n",
    "      \"beam_k\": 2,  \n",
    "      \"beam_steps\": 18,\n",
    "      \"substructure_type\": \"structure\",\n",
    "      \"structure_min_size\": 15,\n",
    "      \"pool_size\": 4,\n",
    "      \"pool_saving_frequency\": 1000,\n",
    "      \"patience\": 5,\n",
    "      \"token_sampling_method\": \"topk\",\n",
    "      \"filter_patience_limit\": 100000\n",
    "    }\n",
    "  },\n",
    "  \"distribution_learning\": {\n",
    "    \"parameters\": {\n",
    "      \"agent\": \"FILL\",  # Only used if transfer_learning is True - use case is transfer learning on an already trained model\n",
    "      \"training_steps\": 20,\n",
    "      \"batch_size\": 512,\n",
    "      \"learning_rate\": 0.0001,\n",
    "      \"training_dataset_path\": \"FILL\",  # Denotes the path to the training dataset of SMILES\n",
    "      \"train_with_randomization\": True,  # If True, the training batch is randomized at every training step\n",
    "      \"transfer_learning\": False\n",
    "    }\n",
    "  },\n",
    "  \"running_mode\": \"goal_directed_generation\",  # Choose between \"distribution_learning\" or \"goal_directed_generation\"\n",
    "  \"model_architecture\": \"mamba\",  # Choose between \"rnn\", \"decoder\", \"mamba\"\n",
    "  \"device\": \"cuda\",  # All models can be run on CPU, in which case, change to \"cpu\". NOTE: Mamba runs will be notably slower on CPU\n",
    "  \"seed\": 0\n",
    "}\n",
    "\n",
    "# Save the configuration file\n",
    "with open(\"config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circus_testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
